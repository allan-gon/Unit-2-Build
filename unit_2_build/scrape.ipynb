{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scrape(title):\n",
    "    # initiates instance of browser\n",
    "    browser = webdriver.Firefox(executable_path='./geckodriver')\n",
    "    #goes to my animelist\n",
    "    browser.get(\"https://myanimelist.net/\")\n",
    "    #click on search bar\n",
    "    search_bar = browser.find_element_by_id(\"topSearchText\")\n",
    "    # make it so i only look at manga\n",
    "    to_manga = browser.find_element_by_id('topSearchValue')\n",
    "    to_manga.click()\n",
    "    '''\n",
    "    # enter search query\n",
    "    search_bar.send_keys(title)\n",
    "    # like pressing enter\n",
    "    search_bar.send_keys(Keys.ENTER)'''\n",
    "    return\n",
    "#base = \"https://myanimelist.net/manga\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_scrape('Berserk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(title):\n",
    "    #fixing the title\n",
    "    title = title.replace(' ','_')\n",
    "    #create url\n",
    "    url = 'site:https://myanimelist.net/manga/' + title\n",
    "    #download the page\n",
    "    client = urlopen(url)\n",
    "    page = client.read()\n",
    "    client.close()\n",
    "    soup = BeautifulSoup(page,'html.parser')\n",
    "    #only take the text\n",
    "    search = ''\n",
    "    for item in soup.select(\"div > span\"):\n",
    "        search += ' ' + item.get_text()\n",
    "    #find relevant text\n",
    "    index = search.index('Score')\n",
    "    looking = search2[index:]\n",
    "    #remove the issue text\n",
    "    looking = looking.replace(\"Ranked: Popularity: Members: Favorites: \",'').split()[:30]\n",
    "    #find the relevant info\n",
    "    score = looking[1]\n",
    "    rank = looking[looking.index('Ranked')+1].strip('#').replace(',','')\n",
    "    popularity = looking[looking.index('Popularity')+1].strip('#').replace(',','')\n",
    "    members = looking[looking.index('Members')+1].replace(',','')\n",
    "    return [score,rank,popularity,members]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#info = scrape(\"Fullmetal Alchemist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The urls follow this pattern \"https://myanimelist.net/manga/*number*/*title*\"\n",
    "\n",
    "Title, though changing is easy to fix because i have aall the titles in a list(elsewhere) but,the number is assigned by the website.\n",
    "\n",
    "In the function I hard coded the 'number' as 25 because for the manga 'Fullmetal Alchemist' that's the website if i tried this scraper with all the animes\n",
    "it wouldn't work.\n",
    "\n",
    "Forinstance, the manga \"Berserk\"'s link is https://myanimelist.net/manga/2/Berserk, as you can see the 'number' portion of the link differs.\n",
    "So my question is how do i account for this?\n",
    "\n",
    "Unrelated, I know there's a cleaner way to scrape using findAll instead of straight get_text() but because of how the website was I decided on skipping that and just using get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disregard anything below this line, past ideas i had to scrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to get score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score = soup.findAll('div',{'data-id':\"info1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score[0].findAll('span',{\"itemprop\":\"ratingValue\"})[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to get Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank = soup.findAll('div',{'data-id':\"info2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank[0].text.split()[1][:-1].strip('#')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to get members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#members = soup.findAll('div',{'class':'spaceit'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#members[5].text.split()[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to get popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''url2 = \"https://myanimelist.net/manga/25/Fullmetal_Alchemist\"\n",
    "client = urlopen(url2)\n",
    "page = client.read()\n",
    "client.close()\n",
    "soup2 = BeautifulSoup(page,'html.parser')\n",
    "search2 = ''\n",
    "for item in soup2.select(\"div > span\"):\n",
    "    search2 += ' ' + item.get_text()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search2.index('Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking = search2[152:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking.replace(\"Ranked: Popularity: Members: Favorites: \",'').split()[:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
